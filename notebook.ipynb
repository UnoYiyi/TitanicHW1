{
  "cells": [
    {
      "metadata": {
        "_uuid": "2a5e25f0f2c6b63f8472797c586d49e58a47e7cf",
        "_cell_guid": "e2f4795f-0571-4f3d-95c6-184c066115ca",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c4657cf0a8ce175a98f59962a66abf564417e785",
        "_cell_guid": "45bf0fb3-4571-4205-b434-6a9b1a50656b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/train.csv\")\ntest  = pd.read_csv(\"../input/test.csv\")\ngendersub = pd.read_csv(\"../input/gender_submission.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba0954befe73fc287652310f3713ebe57be0bc09",
        "_cell_guid": "a668bd44-e5aa-4e91-b21b-1dacc1d0ea4f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.shape, test.shape, gendersub.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81a62d915d40e96180959a28a379066eb52f41f9",
        "_cell_guid": "a8112a17-ed6b-47e0-8721-117f5a603a12"
      },
      "cell_type": "markdown",
      "source": "EDA"
    },
    {
      "metadata": {
        "_uuid": "1d0ee150b71939f4ff79274c29418bc1fae4367f",
        "_cell_guid": "dc19d1f8-0256-4205-9f2d-ec938c3ccc82",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.head(5)\ntest.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f260915e3b7d5a62ddc82725e98debf99ed3dfd7",
        "_cell_guid": "a7859db7-331b-4f1b-8fa6-37136a99032d"
      },
      "cell_type": "markdown",
      "source": "Reading the 4 C's of Data Cleaning from \"https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\", found it useful. Read as followed\n\n\"1. Correcting: Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n2. Completing: There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the modelâ€™s accuracy.\n3. Creating: Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n4. Converting: Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.\"\n\nHow do we correct data? First we need to examine the data, using \n\"print('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')\" - credit to LD Freeman\n\nRun it to test "
    },
    {
      "metadata": {
        "_uuid": "c3277c3e46def1370243d76501672339d31d6a4f",
        "_cell_guid": "c4861d79-9879-4e11-881f-ac84e776fc74",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('Train columns with null values:\\n', train.isnull().sum())\nprint(\"-\"*10)\n      \nprint('Test columns with null values:\\n', test.isnull().sum())\nprint(\"-\"*10)\n      \ntest.describe(include = \"all\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cacc1a53f860fcf55be9bfe7b3c632b68e11125d",
        "_cell_guid": "1c68cdbc-8218-4098-90d5-83649ed0859e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#COMPLETING\ntest.count(),train.count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1d27e5401905c7c0885561a46aa7a4227fe791a3",
        "_cell_guid": "ca6d356c-8a1b-4355-92f6-ac5dee055863",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "train.groupby(['Pclass']).mean()\n#drop values & replace missing values \ntrain = train.drop(['PassengerId','Name','Ticket'], axis = 1)\ntest = test.drop(['Name','Ticket'], axis=1)\ntest[\"Age\"].fillna(test[\"Age\"].mean(), inplace=True)\ntrain[\"Age\"].fillna(train[\"Age\"].mean(), inplace=True)\ntest[\"Fare\"].fillna(test[\"Fare\"].mean(), inplace=True)\n#because cabin miss too many values, so we drop it directly\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis=1)\n#because embarked needs a lot of attention, so we will drop it now (i am running out of time)\ntrain = train.drop(['Embarked'], axis = 1)\ntest = test.drop(['Embarked'], axis=1)\n#change sex to 0 and 1\ntest[\"Sex\"] = test[\"Sex\"].map({\"male\": 0, \"female\":1})\ntrain[\"Sex\"] = train[\"Sex\"].map({\"male\": 0, \"female\":1})\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bc07197173e794dd94ff4cdec45ddf5a4601ff11",
        "_cell_guid": "dd476a84-b3c9-4607-9269-806a8e9231ea",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7a79e41b733aeff1c3079bc937b2150291a1b8f",
        "_cell_guid": "85dfe2c9-acc9-40e2-a4c2-4c8d8282bcc4",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#correlation heatmap\ncorr = test.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aafe2f9dc5c9e567b08b640458a55f31c3a6733d",
        "_cell_guid": "75fac5f3-c39f-4602-99c0-1afd5296d7e6",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#model1 Logistic Regression I found this model on \"https://www.kaggle.com/omarelgabry/a-journey-through-titanic\")\n# define training and testing sets\n\nX_train = train.drop(\"Survived\",axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\",axis=1).copy()\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nlogreg.score(X_train, Y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "974b6399f35fcb183aa8cb141a128e72cde4ae99",
        "_cell_guid": "57dec5a8-0aff-4f5e-960a-8565bec59db2",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Model2 Random Forests\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train) \n## maybe overfit",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cfded0007e0fbb4269135b03de739357c663dbcf",
        "_cell_guid": "a95e64d1-2487-41f6-9242-5d32e4387a5c",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#print submission\nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv(\"titanichw1.csv\",index = False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}